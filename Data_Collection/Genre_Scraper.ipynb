{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30268,
     "status": "ok",
     "timestamp": 1572047259486,
     "user": {
      "displayName": "Isaac Schaal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBIjjjnz_rTtosOMO8OTVjEKf3YbMgYn2Rzd72pPNE=s64",
      "userId": "04947858529345876410"
     },
     "user_tz": 420
    },
    "id": "9rGGe3t8cZW4",
    "outputId": "e1804372-c198-49bb-b6a6-6d2e81023785"
   },
   "outputs": [],
   "source": [
    "## This is a script for scraping from Wikiart, based on either style or genre\n",
    "## It was written by Robbie Barrat\n",
    "## https://github.com/robbiebarrat/art-DCGAN/blob/master/genre-scraper.py\n",
    "\n",
    "## It was modified to run as an .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wk5HrmVaVu08"
   },
   "outputs": [],
   "source": [
    "# Updated/fixed version from Gene Kogan's \"machine learning for artists\" collection - ml4a.github.io\n",
    "# Huge shoutout to Gene Kogan for fixing this script - a second time - hahaha.\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import urllib\n",
    "import urllib.request\n",
    "import itertools\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool\n",
    "\n",
    "def get_painting_list(count, typep, searchword):\n",
    "    try:\n",
    "        time.sleep(3.0*random.random())  # random sleep to decrease concurrence of requests\n",
    "        url = \"https://www.wikiart.org/en/paintings-by-%s/%s/%d\"%(typep, searchword, count)\n",
    "        soup = BeautifulSoup(urllib.request.urlopen(url), \"lxml\")\n",
    "        regex = r'https?://uploads[0-9]+[^/\\s]+/\\S+\\.jpg'\n",
    "        url_list = re.findall(regex, str(soup.html()))\n",
    "        count += len(url_list)\n",
    "        return url_list\n",
    "    except Exception as e:\n",
    "        print('failed to scrape %s'%url, e)\n",
    "\n",
    "\n",
    "def downloader(link, genre, output_dir):\n",
    "    global num_downloaded, num_images\n",
    "    item, file = link\n",
    "    filepath = file.split('/')\n",
    "    #savepath = '%s/%s/%d_%s' % (output_dir, genre, item, filepath[-1])\n",
    "    savepath = '%s/%s/%s' % (output_dir, genre, filepath[-1])    \n",
    "    try:\n",
    "        time.sleep(0.2)  # try not to get a 403\n",
    "        urllib.request.urlretrieve(file, savepath)\n",
    "        num_downloaded += 1\n",
    "        if num_downloaded % 100 == 0:\n",
    "            print('downloaded number %d / %d...' % (num_downloaded, num_images))\n",
    "    except Exception as e:\n",
    "        print(\"failed downloading \" + str(file), e) \n",
    "\n",
    "\n",
    "def main(typep, searchword, num_pages, output_dir):\n",
    "    global num_images\n",
    "    print('gathering links to images... this may take a few minutes')\n",
    "    threadpool = Pool(multiprocessing.cpu_count()) ## CHANGED!\n",
    "    numbers = list(range(1, num_pages))\n",
    "    wikiart_pages = threadpool.starmap(get_painting_list, zip(numbers, itertools.repeat(typep), itertools.repeat(searchword))) \n",
    "    threadpool.close()\n",
    "    threadpool.join()\n",
    "\n",
    "    pages = [page for page in wikiart_pages if page ]\n",
    "    items = [item for sublist in pages for item in sublist]\n",
    "    items = list(set(items))  # get rid of duplicates\n",
    "    num_images = len(items)\n",
    "    \n",
    "    if not os.path.isdir('%s/%s'%(output_dir, searchword)):\n",
    "        os.mkdir('%s/%s'%(output_dir, searchword))\n",
    "    \n",
    "    print('attempting to download %d images'%num_images)\n",
    "    threadpool = Pool(multiprocessing.cpu_count()-1)\n",
    "    threadpool.starmap(downloader, zip(enumerate(items), itertools.repeat(searchword), itertools.repeat(output_dir)))\n",
    "    threadpool.close    \n",
    "    threadpool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2049160,
     "status": "ok",
     "timestamp": 1572036165934,
     "user": {
      "displayName": "Isaac Schaal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBIjjjnz_rTtosOMO8OTVjEKf3YbMgYn2Rzd72pPNE=s64",
      "userId": "04947858529345876410"
     },
     "user_tz": 420
    },
    "id": "0qbRlJelV1zS",
    "outputId": "7c61929d-08e6-4291-c362-b242f6bc621c"
   },
   "outputs": [],
   "source": [
    "searchword = \"impressionism\"\n",
    "output_dir = \"data\"\n",
    "\n",
    "num_downloaded = 0\n",
    "num_images = 0\n",
    "\n",
    "if not os.path.isdir('%s/%s'%(output_dir, searchword)):     \n",
    "    os.mkdir('%s/%s'%(output_dir, searchword))\n",
    "\n",
    "num_pages = 1000\n",
    "main('style', searchword, num_pages, output_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
